{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97172e6b-4b9b-4cf3-9a0d-81f724ca2b71",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates how to implement parallel function calling using Anthropic Claude Sonnet on Amazon Bedrock using the Boto3 API\n",
    "\n",
    "It builds upon the multi agent implementation example from [here](bedrock-converse-multi-agent.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ef159f73-c12d-4277-88e1-3b9411c3502d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install python-dotenv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "8e954747-91e5-48e9-8e16-9e12ee92d0c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "import concurrent.futures\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Load .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc201762-cac4-4a89-abd1-f7766882e2a8",
   "metadata": {},
   "source": [
    "## Define tools for the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d4253e82-4eca-4e65-a4a7-f24e60ae03f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OPENWEATHERMAP_API_KEY = os.getenv(\"OPENWEATHERMAP_API_KEY\")\n",
    "\n",
    "def get_current_weather(input) -> int:\n",
    "    \"\"\"Get the current temperature from a city, in Fahrenheit\"\"\"\n",
    "    \n",
    "    city = input[\"city\"]\n",
    "    country = input [\"country\"]\n",
    "    response = requests.get(\n",
    "        f\"http://api.openweathermap.org/data/2.5/weather?q={city},{country}&appid={OPENWEATHERMAP_API_KEY}\"\n",
    "    )\n",
    "    data = response.json()\n",
    "    temp_kelvin = data[\"main\"][\"temp\"]\n",
    "    temp_fahrenheit = (temp_kelvin - 273.15) * 9 / 5 + 32\n",
    "    return int(temp_fahrenheit)\n",
    "\n",
    "\n",
    "def get_difference(input) -> int:\n",
    "    \"\"\"Get the difference between two numbers\"\"\"\n",
    "    \n",
    "    minuend = input[\"minuend\"]\n",
    "    subtrahend = input[\"subtrahend\"]\n",
    "    return minuend - subtrahend\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current temperature from a city, in Fahrenheit\",\n",
    "            # \"description\": \"This tool returns the temperature of city in fahrenheit\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"city\": {\n",
    "                            \"type\": \"string\",\n",
    "                            # \"description\": \"The city\"\n",
    "                            \"description\": \"City\"\n",
    "                        },\n",
    "                        \"country\": {\n",
    "                            \"type\": \"string\",\n",
    "                            # \"description\": \"The country code\"\n",
    "                            \"description\": \"Country Code\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"city\", \"country\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"get_difference\",\n",
    "            \"description\": \"Get the difference between two numbers\",\n",
    "            # \"description\": \"This tool substract and returns the difference between two numbers\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"minuend\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The number from which another number is to be subtracted\"\n",
    "                        },\n",
    "                        \"subtrahend\": {\n",
    "                            \"type\": \"integer\",\n",
    "                            \"description\": \"The number to be subtracted\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"minuend\", \"subtrahend\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc2dae-cd1f-4990-9671-7b7a4f6cee45",
   "metadata": {},
   "source": [
    "## Define helper function to call Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "503348e1-6a82-4c57-bccf-c1cda34d8fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def call_bedrock(system_message, message_list, tool_list):\n",
    "    session = boto3.Session()\n",
    "\n",
    "    bedrock = session.client(service_name='bedrock-runtime')\n",
    "    \n",
    "    response = bedrock.converse(\n",
    "        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "        messages=message_list,\n",
    "        system=system_message,\n",
    "        inferenceConfig={\n",
    "            # \"maxTokens\": 2000,\n",
    "            \"temperature\": 0\n",
    "        },\n",
    "        toolConfig={ \"tools\": tool_list }\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cd207-a6ae-458f-8f83-46e25880b73a",
   "metadata": {},
   "source": [
    "## Define helper function to invoke a given tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e2392de1-d4c4-416b-8f4d-eb5f12b8a5be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_tool_result(tool_use_block):\n",
    "    \n",
    "    tool_name = tool_use_block['name']\n",
    "    tool_input = tool_use_block['input']\n",
    "    \n",
    "    print(f\"Using tool `{tool_name}` for query `{tool_input}`\")\n",
    "    \n",
    "    func = globals()[tool_name]\n",
    "    try:\n",
    "        tool_result_value = func(tool_input)\n",
    "        if tool_result_value is not None:\n",
    "            return {\n",
    "                \"toolResult\": {\n",
    "                    \"toolUseId\": tool_use_block['toolUseId'],\n",
    "                    \"content\": [\n",
    "                        { \"json\": { \"text\": tool_result_value } }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return { \n",
    "            \"toolResult\": {\n",
    "                \"toolUseId\": tool_use_block['toolUseId'],\n",
    "                \"content\": [  { \"text\": repr(e) } ],\n",
    "                \"status\": \"error\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad0b591-0d47-4a47-bd0b-4134ddb36745",
   "metadata": {},
   "source": [
    "## Define function to handle the raw responses from Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5e425c6a-e0c0-4698-ac98-8b26811e2b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_model_response(response):\n",
    "    \n",
    "    response_content_blocks = response['content']\n",
    "    follow_up_content_blocks = []\n",
    "    tool_use_blocks = []\n",
    "    \n",
    "    for content_block in response_content_blocks:\n",
    "        if 'toolUse' in content_block:\n",
    "            tool_use_blocks.append(content_block['toolUse'])\n",
    "    \n",
    "    if len(tool_use_blocks) > 0:\n",
    "            if len(tool_use_blocks) == 1:\n",
    "                \n",
    "                # If there's only one tool use, process it without using parallel execution\n",
    "                tool_result_value = get_tool_result(tool_use_blocks[0])\n",
    "                follow_up_content_blocks.append(tool_result_value)\n",
    "            \n",
    "            else:\n",
    "                # If there are multiple elements, use parallel execution\n",
    "                with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                    # Submit tasks for each item in the input list\n",
    "                    future_results = [executor.submit(get_tool_result, tool_use_block) for tool_use_block in tool_use_blocks]\n",
    "\n",
    "                    # Collect results as they complete\n",
    "                    for future in concurrent.futures.as_completed(future_results):\n",
    "                        result = future.result()\n",
    "                        follow_up_content_blocks.append(result)\n",
    "         \n",
    "    if len(follow_up_content_blocks) > 0:\n",
    "        \n",
    "        follow_up_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": follow_up_content_blocks,\n",
    "        }\n",
    "        \n",
    "        return follow_up_message\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80a4cd4-b6f3-4be5-b516-fd2c7734b0cb",
   "metadata": {},
   "source": [
    "## Define function that implements the Agent Loop till final response is received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "00642d4f-c8e0-4acb-a241-2de3bfe6cd08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_agent(system_prompt, input_prompt, tool_list):\n",
    "    # MAX_LOOPS = 6\n",
    "    # loop_count = 0\n",
    "    continue_loop = True\n",
    "    output = \"\"\n",
    "    \n",
    "    system_message = [\n",
    "        {\n",
    "            \"text\": system_prompt\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    message_list = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [ { \"text\": input_prompt } ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    while continue_loop:\n",
    "        response = call_bedrock(system_message, message_list, tool_list)\n",
    "\n",
    "        response_message = response['output']['message']\n",
    "        message_list.append(response_message)\n",
    "\n",
    "        # loop_count = loop_count + 1\n",
    "\n",
    "        # if loop_count >= MAX_LOOPS:\n",
    "        #     print(f\"Hit loop limit: {loop_count}\")\n",
    "        #     break\n",
    "\n",
    "        follow_up_message = handle_model_response(response_message)\n",
    "\n",
    "        if follow_up_message is None:\n",
    "            # No remaining work to do, return final response to user\n",
    "            continue_loop = False\n",
    "            if response['stopReason'] == \"end_turn\":\n",
    "                agent_output = response_message['content'][0]['text']\n",
    "        else:\n",
    "            message_list.append(follow_up_message)\n",
    "            \n",
    "    return message_list, agent_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de16d9c6-4f90-4ae5-8f41-cea1b1c8fb8a",
   "metadata": {},
   "source": [
    "## Define agent executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b6658274-9a9c-4c18-82d5-b88fef317e18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agent Executor\n",
    "\n",
    "def agent_executor(agent_input_prompt):\n",
    "    \n",
    "    agent_system_prompt = \"You are a helpful assistant that answer questions about weather\"\n",
    "\n",
    "    agent_workflow, agent_output = run_agent(agent_system_prompt, agent_input_prompt, tools)\n",
    "    \n",
    "    # returning just the output for now\n",
    "    # can choose to return the workflow `agent_workflow` as well which includes details of the different tools called when this agent is called\n",
    "    \n",
    "    return agent_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a0c02-b7e2-4384-aa10-be0b090ff1ee",
   "metadata": {},
   "source": [
    "## Run Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "3cc60880-d4fe-40c3-8105-72887ca4498c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tool `get_current_weather` for query `{'city': 'Austin', 'country': 'US'}`\n",
      "Using tool `get_current_weather` for query `{'city': 'Tokyo', 'country': 'JP'}`\n",
      "Using tool `get_current_weather` for query `{'city': 'Seattle', 'country': 'US'}`\n",
      "Using tool `get_difference` for query `{'minuend': 89, 'subtrahend': 80}`\n",
      "Using tool `get_difference` for query `{'minuend': 89, 'subtrahend': 63}`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Austin at 89°F is the warmest of the three cities. It is 9°F warmer than Tokyo, and 26°F warmer than Seattle.'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor(\"Where is it warmest: Austin, Texas; Tokyo; or Seattle? And by how much is it warmer than the other cities?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
