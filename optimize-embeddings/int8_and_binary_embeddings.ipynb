{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c56efea-5831-48db-8ad8-dd962cf67d78",
   "metadata": {},
   "source": [
    "# Binary and int8 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b28c2c-85aa-4c59-b79e-473c12673e91",
   "metadata": {},
   "source": [
    "## Setup and Introduction\n",
    "This notebook demonstrates a comparison between three types of embeddings: float embeddings, int8 embeddings, and binary embeddings. We use the Cohere embed model on Amazon Bedrock to generate these embeddings and compare their memory footprint and retrieval speed using FAISS (Facebook AI Similarity Search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df708bfc-829b-4395-937f-4a03d28cc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2722df-dee1-425a-a399-b6d497dabcc5",
   "metadata": {},
   "source": [
    "## Setup Bedrock Client\n",
    "\n",
    "The `invoke_bedrock` function is set up to interact with Amazon Bedrock's API. It sends requests to generate embeddings for given texts using the specified embedding types.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d9f541e0-b076-414d-b7aa-0991dad05322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "bedrock = boto3.client(service_name=\"bedrock\")\n",
    "bedrock_runtime = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "def invoke_bedrock(texts, input_type, embedding_types, model_id=\"cohere.embed-english-v3\"):\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body= json.dumps({\n",
    "            \"texts\": texts,\n",
    "            \"input_type\": input_type,\n",
    "            \"truncate\": \"END\",\n",
    "            \"embedding_types\": embedding_types\n",
    "        }),\n",
    "    \tmodelId=model_id,\n",
    "        accept=\"application/json\", \n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    embedding_output = response_body.get(\"embeddings\")\n",
    "    response_type = response_body.get(\"response_type\")\n",
    "    \n",
    "    return embedding_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5259bce-960f-4d8e-9edb-a6798bb86d10",
   "metadata": {},
   "source": [
    "## Setup dataset\n",
    "\n",
    "We use the MSMARCO dataset, a large-scale dataset for information retrieval tasks. We load a subset of this dataset, extracting passages and queries for our embedding experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "444507d3-0fcd-4d5f-8e0e-3293fc5cea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the MSMARCO dataset from Hugging Face\n",
    "dataset = load_dataset(\"ms_marco\", \"v2.1\", split=\"train[:96]\")\n",
    "passages = dataset['passages']\n",
    "corpus = [ passage[\"passage_text\"][0] for passage in passages ]\n",
    "queries = dataset['query']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1369c6-dda5-434f-bb9d-bb50a908f657",
   "metadata": {},
   "source": [
    "## Vector Embedding Memory Footprint\n",
    "\n",
    "This section demonstrates how different embedding types affect memory usage.\n",
    "\n",
    "The `calculate_embedding_memory` function computes the memory footprint of embeddings based on their type (float, int8, or binary). It calculates the total size, embedding dimension, number of embeddings, and size of each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9bef857e-8a14-45ed-a664-0d95b8b126de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_embedding_memory(embeddings, embed_type=\"float\"):\n",
    "\n",
    "    if embed_type == \"float\":\n",
    "        data_type=np.float32\n",
    "    elif embed_type == \"int8\":\n",
    "        data_type=np.int8\n",
    "    elif embed_type == \"ubinary\":\n",
    "        data_type=np.uint8\n",
    "\n",
    "    embeddings_np = np.array(embeddings).astype(data_type)\n",
    "    num_embeddings, embedding_dim = embeddings_np.shape\n",
    "    \n",
    "    # Get the size of each element in bytes\n",
    "    element_size = embeddings_np.itemsize\n",
    "    \n",
    "    # Calculate the total size\n",
    "    total_size = num_embeddings * embedding_dim * element_size\n",
    "    \n",
    "    return total_size, embedding_dim, num_embeddings, element_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429f4d6-d534-40f5-9bb9-2df06691f3e8",
   "metadata": {},
   "source": [
    "### Float embeddings\n",
    "\n",
    "Float embeddings are the standard output of most embedding models. They use 32 bits (4 bytes) per dimension, providing high precision but requiring more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "010142e3-7450-4b2f-8ffe-318a70e8a4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 393216 bytes\n",
      "Total Size (MB): 0.375 MB\n",
      "Size of each embedding dimension: 4 byte(s)\n",
      "Embeddings dimension: 1024\n",
      "Number of embeddings: 96\n"
     ]
    }
   ],
   "source": [
    "embed_type=\"float\"\n",
    "response= invoke_bedrock(corpus, input_type=\"search_document\", embedding_types=[embed_type])\n",
    "embeddings_float = response[embed_type]\n",
    "\n",
    "# Calculate memory\n",
    "total_size, embedding_dim, num_embeddings, element_size = calculate_embedding_memory(embeddings_float, embed_type)\n",
    "\n",
    "print(f\"Total Size: {total_size} bytes\")\n",
    "print(f\"Total Size (MB): {total_size / (1024 * 1024)} MB\")\n",
    "print(f\"Size of each embedding dimension: {element_size} byte(s)\")\n",
    "print(f\"Embeddings dimension: {embedding_dim}\")\n",
    "print(f\"Number of embeddings: {num_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b954e6-83e2-43b1-b804-a6d7788dd512",
   "metadata": {},
   "source": [
    "### int8 embeddings\n",
    "\n",
    "Int8 embeddings quantize the float embeddings to 8-bit integers. This reduces memory usage to 1 byte per dimension, potentially sacrificing some precision for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9d439af9-06e8-4ff2-ba8a-f3f876d9ca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 98304 bytes\n",
      "Total Size (MB): 0.09375 MB\n",
      "Size of each embedding dimension: 1 byte(s)\n",
      "Embeddings dimension: 1024\n",
      "Number of embeddings: 96\n"
     ]
    }
   ],
   "source": [
    "embed_type=\"int8\"\n",
    "response= invoke_bedrock(corpus, input_type=\"search_document\", embedding_types=[embed_type])\n",
    "embeddings_int8 = response[embed_type]\n",
    "\n",
    "# Calculate memory\n",
    "total_size, embedding_dim, num_embeddings, element_size = calculate_embedding_memory(embeddings_int8, embed_type)\n",
    "\n",
    "print(f\"Total Size: {total_size} bytes\")\n",
    "print(f\"Total Size (MB): {total_size / (1024 * 1024)} MB\")\n",
    "print(f\"Size of each embedding dimension: {element_size} byte(s)\")\n",
    "print(f\"Embeddings dimension: {embedding_dim}\")\n",
    "print(f\"Number of embeddings: {num_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db5743b-fd84-4e75-b5cb-4ddc237fc5eb",
   "metadata": {},
   "source": [
    "### Binary embeddings\n",
    "\n",
    "Binary embeddings further compress the representation to 1 bit per dimension. While this dramatically reduces memory usage, it may lead to a more significant loss in precision compared to float or int8 embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9fb98626-6142-4b44-b2d2-7e47f498f213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size: 12288 bytes\n",
      "Total Size (MB): 0.01 MB\n",
      "Size of each embedding dimension: 1 byte(s)\n",
      "Embeddings dimension: 128\n",
      "Number of embeddings: 96\n"
     ]
    }
   ],
   "source": [
    "embed_type=\"ubinary\"\n",
    "response= invoke_bedrock(corpus, input_type=\"search_document\", embedding_types=[embed_type])\n",
    "embeddings_binary = response[embed_type]\n",
    "\n",
    "# Calculate memory\n",
    "total_size, embedding_dim, num_embeddings, element_size = calculate_embedding_memory(embeddings_binary, embed_type)\n",
    "\n",
    "print(f\"Total Size: {total_size} bytes\")\n",
    "print(f\"Total Size (MB): {total_size / (1024 * 1024):.2f} MB\")\n",
    "print(f\"Size of each embedding dimension: {element_size} byte(s)\")\n",
    "print(f\"Embeddings dimension: {embedding_dim}\")\n",
    "print(f\"Number of embeddings: {num_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32b417-9fde-46fe-af4c-230d52bfdc48",
   "metadata": {},
   "source": [
    "## Retrieval Speed\n",
    "\n",
    "This section compares the retrieval speed of different embedding types using FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e4dc8ed8-81b2-4aa4-86df-60083c5afb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /opt/conda/lib/python3.10/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from faiss-cpu) (23.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0ad9b-1688-417c-9a68-c52a36d4ecdf",
   "metadata": {},
   "source": [
    "### float embeddings\n",
    "\n",
    "We create a FAISS index using float embeddings and perform a search. This serves as a baseline for comparison with other embedding types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "5238844b-b3ba-4087-a998-6a23f3f8fe7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for retrieval: 0.000089 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import time\n",
    "\n",
    "embed_type=\"float\"\n",
    "\n",
    "#Cast embeddings to numpy\n",
    "embeddings_fl = np.array(embeddings_float).astype('float32')\n",
    "\n",
    "#Add the embeddings to the faiss index\n",
    "num_dim = 1024   #Use 1024 dimensions for the embed-english-v3.0\n",
    "index = faiss.IndexFlatL2(num_dim)\n",
    "index.add(embeddings_fl)\n",
    "\n",
    "# Search in your index\n",
    "query = queries[0]\n",
    "response= invoke_bedrock([query], input_type=\"search_query\", embedding_types=[embed_type])\n",
    "query_emb = np.array(response[embed_type]).astype('float32')\n",
    "\n",
    "start_time = time.time()\n",
    "hits_scores, hits_doc_ids = index.search(query_emb, k=min(10*5, index.ntotal))\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time_secs = f\"{elapsed_time:.6f} seconds\"\n",
    "hits, time = search(index, query, embed_type)\n",
    "print(f\"Time taken for retrieval: {time}\\n\\n\")\n",
    "# for hit in hits:\n",
    "#     print(f\"{hit['score']:.2f}\", corpus[hit['doc_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cf4a2b-b78f-4241-99c4-0ba3fe27bf61",
   "metadata": {},
   "source": [
    "### int8 embeddings\n",
    "\n",
    "Int8 embeddings are used to create a quantized FAISS index. The search performance is compared to the float embeddings baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c4cd6e4e-0ae5-4d7c-886b-00496061f110",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Wrong number or type of arguments for overloaded function 'new_IndexHNSWSQ'.\n  Possible C/C++ prototypes are:\n    faiss::IndexHNSWSQ::IndexHNSWSQ()\n    faiss::IndexHNSWSQ::IndexHNSWSQ(int,faiss::ScalarQuantizer::QuantizerType,int,faiss::MetricType)\n    faiss::IndexHNSWSQ::IndexHNSWSQ(int,faiss::ScalarQuantizer::QuantizerType,int)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4965/3552962883.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#Add the embeddings to the faiss index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnum_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m   \u001b[0;31m#Use 1024 dimensions for the embed-english-v3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mquantizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalarQuantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mScalarQuantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQT_8bit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexHNSWSQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_int8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Search in your index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/faiss/swigfaiss_avx512.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   6753\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6754\u001b[0;31m         \u001b[0m_swigfaiss_avx512\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexHNSWSQ_swiginit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_swigfaiss_avx512\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_IndexHNSWSQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Wrong number or type of arguments for overloaded function 'new_IndexHNSWSQ'.\n  Possible C/C++ prototypes are:\n    faiss::IndexHNSWSQ::IndexHNSWSQ()\n    faiss::IndexHNSWSQ::IndexHNSWSQ(int,faiss::ScalarQuantizer::QuantizerType,int,faiss::MetricType)\n    faiss::IndexHNSWSQ::IndexHNSWSQ(int,faiss::ScalarQuantizer::QuantizerType,int)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "embed_type=\"int8\"\n",
    "\n",
    "#Cast embeddings to numpy\n",
    "embeddings_int8 = np.array(embeddings_int8).astype('int8')\n",
    "\n",
    "#Add the embeddings to the faiss index\n",
    "num_dim = 1024   #Use 1024 dimensions for the embed-english-v3.0\n",
    "quantizer = faiss.ScalarQuantizer(num_dim, faiss.ScalarQuantizer.QT_8bit)  \n",
    "index = faiss.IndexHNSWSQ(num_dim, quantizer, 256)\n",
    "index.add(embeddings_int8)\n",
    "\n",
    "# Search in your index\n",
    "query = queries[0]\n",
    "response= invoke_bedrock([query], input_type=\"search_query\", embedding_types=[embed_type])\n",
    "query_emb = np.array(response[embed_type]).astype('int8')\n",
    "\n",
    "start_time = time.time()\n",
    "hits_scores, hits_doc_ids = index.search(query_emb, k=min(10*5, index.ntotal))\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time_secs = f\"{elapsed_time:.6f} seconds\"\n",
    "hits, time = search(index, query, embed_type)\n",
    "print(f\"Time taken for retrieval: {time}\\n\\n\")\n",
    "\n",
    "# for hit in hits:\n",
    "#     print(f\"{hit['score']:.2f}\", corpus[hit['doc_id']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1e1dcb-1171-4943-b432-f9bbf3715cc0",
   "metadata": {},
   "source": [
    "### Binary embeddings\n",
    "\n",
    "A binary FAISS index is created using binary embeddings. The search performance is again compared to the other embedding types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dc0ff3de-d518-4cb5-9ba0-b9e6380d804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for retrieval: 0.000064 seconds\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import time\n",
    "\n",
    "embed_type=\"ubinary\"\n",
    "\n",
    "#Cast embeddings to numpy\n",
    "embeddings_bin = np.array(embeddings_binary).astype('uint8')\n",
    "\n",
    "#Add the embeddings to the faiss index\n",
    "num_dim = 1024   #Use 1024 dimensions for the embed-english-v3.0\n",
    "index = faiss.IndexBinaryFlat(num_dim)\n",
    "index.add(embeddings_bin)\n",
    "\n",
    "# Search in your index\n",
    "query = queries[0]\n",
    "response= invoke_bedrock([query], input_type=\"search_query\", embedding_types=[embed_type])\n",
    "query_emb = np.array(response[embed_type]).astype('uint8')\n",
    "\n",
    "start_time = time.time()\n",
    "hits_scores, hits_doc_ids = index.search(query_emb, k=min(10*5, index.ntotal))\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "elapsed_time_secs = f\"{elapsed_time:.6f} seconds\"\n",
    "hits, time = search(index, query, embed_type)\n",
    "print(f\"Time taken for retrieval: {time}\\n\\n\")\n",
    "# for hit in hits:\n",
    "#     print(f\"{hit['score']:.2f}\", corpus[hit['doc_id']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
