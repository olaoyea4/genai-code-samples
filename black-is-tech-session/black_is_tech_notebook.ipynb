{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88a5ab2f-d044-4956-b75b-7408d9c3e323",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation with Amazon Bedrock - Enhancing Chat Applications with RAG\n",
    "\n",
    "> *PLEASE NOTE: This notebook should work well with the **`Data Science 3.0`** kernel in SageMaker Studio*\n",
    "\n",
    "---\n",
    "\n",
    "## Chat with LLMs Overview\n",
    "\n",
    "Conversational interfaces such as chatbots and virtual assistants can be used to enhance the user experience for your customers. Chatbots can be used in a variety of applications, such as customer service, sales, and e-commerce, to provide quick and efficient responses to users.\n",
    "\n",
    "The key technical detail which we need to include in our system to enable a chat feature is conversational memory. This way, customers can ask follow up questions and the LLM will understand what the customer has already said in the past. The image below shows how this is orchestrated at a high level.\n",
    "\n",
    "![Amazon Bedrock - Conversational Interface](./images/chatbot_bedrock.png)\n",
    "\n",
    "## Extending Chat with RAG\n",
    "\n",
    "However, in our workshop's situation, we want to be able to enable a customer to ask follow up questions regarding documentation we provide through RAG. This means we need to build a system which has conversational memory AND contextual retrieval built into the text generation.\n",
    "\n",
    "![4](./images/context-aware-chatbot.png)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf5603",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup `boto3` Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7518733a",
   "metadata": {},
   "source": [
    "#### Libraries needed for the installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a6a9b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain>=0.1.11 in /opt/conda/lib/python3.10/site-packages (0.1.13)\n",
      "Requirement already satisfied: transformers<5,>=4.24 in /opt/conda/lib/python3.10/site-packages (4.40.2)\n",
      "Requirement already satisfied: faiss-cpu<2,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: pypdf<4,>=3.8 in /opt/conda/lib/python3.10/site-packages (3.17.4)\n",
      "Requirement already satisfied: pinecone-client==2.2.4 in /opt/conda/lib/python3.10/site-packages (2.2.4)\n",
      "Requirement already satisfied: apache-beam==2.52. in /opt/conda/lib/python3.10/site-packages (2.52.0)\n",
      "Requirement already satisfied: tiktoken==0.5.2 in /opt/conda/lib/python3.10/site-packages (0.5.2)\n",
      "Requirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.10/site-packages (7.8.2)\n",
      "Requirement already satisfied: matplotlib==3.8.2 in /opt/conda/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: anthropic==0.9.0 in /opt/conda/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: llama-index==0.9.0 in /opt/conda/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (4.12.2)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (2.6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (2.9.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (1.26.19)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from pinecone-client==2.2.4) (1.24.4)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.7)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (3.10.4)\n",
      "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.3.1.1)\n",
      "Requirement already satisfied: cloudpickle~=2.2.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2.2.1)\n",
      "Requirement already satisfied: fastavro<2,>=0.23.6 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.9.5)\n",
      "Requirement already satisfied: fasteners<1.0,>=0.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.19)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.59.3)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2.7.3)\n",
      "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.22.0)\n",
      "Requirement already satisfied: js2py<1,>=0.74 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.74)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (4.17.3)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.6.1)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (23.2)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (4.8.0)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (4.25.3)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (1.4.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2023.3)\n",
      "Requirement already satisfied: regex>=2020.6.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (2024.5.15)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.22.0)\n",
      "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (11.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix<1 in /opt/conda/lib/python3.10/site-packages (from apache-beam==2.52.) (0.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib==3.8.2) (3.1.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (4.4.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (1.10.16)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (1.3.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /opt/conda/lib/python3.10/site-packages (from anthropic==0.9.0) (0.19.1)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.0) (2.0.30)\n",
      "Requirement already satisfied: aiostream<0.6.0,>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (4.12.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (0.5.14)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (1.2.14)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (2023.6.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (1.6.0)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (1.35.13)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (2.2.2)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (8.4.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from llama-index==0.9.0) (0.9.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (3.9.5)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (4.0.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.29 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (0.0.38)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (0.1.52)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (0.0.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain>=0.1.11) (0.1.82)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers<5,>=4.24) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers<5,>=4.24) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5,>=4.24) (0.4.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (0.2.2)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.7 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (3.6.7)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (8.25.0)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ipywidgets<8,>=7) (1.1.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.1.11) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic==0.9.0) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->anthropic==0.9.0) (1.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.12.2->llama-index==0.9.0) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.6.0,>=0.5.7->llama-index==0.9.0) (3.21.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.9.3->llama-index==0.9.0) (1.14.1)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.52.) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam==2.52.) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic==0.9.0) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->anthropic==0.9.0) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->anthropic==0.9.0) (0.14.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (0.6.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=4.0.0->ipywidgets<8,>=7) (4.9.0)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /opt/conda/lib/python3.10/site-packages (from js2py<1,>=0.74->apache-beam==2.52.) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from js2py<1,>=0.74->apache-beam==2.52.) (2.7.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.1.11) (3.0.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam==2.52.) (0.20.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.0) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.0) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.3.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.0) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect>=0.8.0->llama-index==0.9.0) (1.0.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (7.1.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->llama-index==0.9.0) (2024.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets<8,>=7) (0.8.4)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.10.0)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.24.0)\n",
      "Requirement already satisfied: jupyterlab<4.2,>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (4.1.6)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.2.4)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (6.4.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8,>=7) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets<8,>=7) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8,>=7) (0.2.2)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (23.1.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (3.1.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (8.6.2)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.6.3)\n",
      "Requirement already satisfied: jupyter-server-terminals in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (5.10.4)\n",
      "Requirement already satisfied: overrides in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.20.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (26.0.3)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.18.1)\n",
      "Requirement already satisfied: websocket-client in /opt/conda/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.0.4)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (6.29.4)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.2.5)\n",
      "Requirement already satisfied: tomli>=1.2.2 in /opt/conda/lib/python3.10/site-packages (from jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.0.1)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.14.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.9.25)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/conda/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.8.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ipykernel>=6.5.0->jupyterlab<4.2,>=4.1.1->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (5.9.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.1.5)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/conda/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (4.2.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.1.1)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.10.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/conda/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.3.0)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/conda/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.20.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (21.2.0)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (20.11.0)\n",
      "Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (24.6.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.22)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=3.2.0->jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=4.4.1->widgetsnbextension~=3.6.7->ipywidgets<8,>=7) (2.9.0.20240316)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install  \\\n",
    "    \"langchain>=0.1.11\" \\\n",
    "    \"transformers>=4.24,<5\" \\\n",
    "    \"faiss-cpu>=1.7.4,<2\" \\\n",
    "    \"pypdf>=3.8,<4\" \\\n",
    "    pinecone-client==2.2.4 \\\n",
    "    apache-beam==2.52. \\\n",
    "    tiktoken==0.5.2 \\\n",
    "    \"ipywidgets>=7,<8\" \\\n",
    "    matplotlib==3.8.2 \\\n",
    "    anthropic==0.9.0 \\\n",
    "    llama-index==0.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba54e478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "region = os.environ.get(\"AWS_REGION\")\n",
    "boto3_bedrock = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d433f8ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6380876",
   "metadata": {},
   "source": [
    "### Use the following COT prompt to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563c1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Human: You are a supply chain inspector. Your job is to assess the risk of a supplier. You measure the supplier risk by evaluating the supplier across three dimensions: country, size, and reputation.\\nCountry - north America and west Europe countries are considered low risk. The rest of the world is considered medium risk.\\nSize - supplier with over 1000 employees is low risk. Supplier with 50 to 999 employees is medium risk, and a supplier with under 50 employees is high risk.\\nReputation - reputation scores are between 1 to 10 where a score of 1 to 3 is low risk, a score of 4 to 7 is medium risk, and a reputation score of 8 to 10 is high risk.\\nThe risk formula is to take the maximum risk across the three dimensions.\\n##\\nExample:\\nSupplier: A\\nCountry: Chad\\nSize: 30\\nReputation: 8\\nLet's think step by step:\\nChad is not in North America or West Europe therefore country risk is medium.\\nA size of 30 is below 50 and therefore considered high risk.\\nA reputation score of 8 is between 8 to 10 and therefore considered high risk.\\nFinal Answer taking the maximum risk among all: Supplier A is at High risk.\\n##\\nSupplier: B\\nCountry: USA\\nSize: 40\\nReputation: 2\\nLet's think step by step: \\nAssistant: \"\n",
    "\n",
    "\n",
    "model_output = \" Okay, let's evaluate Supplier B:\\n\\n- Country: USA is in North America, so the country risk is low.\\n\\n- Size: 40 employees is below 50, so the size risk is high. \\n\\n- Reputation: A score of 2 is between 1-3, so the reputation risk is low.\\n\\nTo determine the overall risk, we take the maximum risk across the three dimensions. \\n\\nThe maximum risk for Supplier B is high due to its small size.\\n\\nTherefore, my assessment is that Supplier B is at high risk overall.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100c1f4a-d295-478a-b2df-487b67c0472d",
   "metadata": {},
   "source": [
    "---\n",
    "## Using LangChain for Conversation Memory\n",
    "\n",
    "We will use LangChain's `ConversationBufferMemory` class provides an easy way to capture conversational memory for LLM chat applications. Let's check out an example of Claude being able to retrieve context through conversational memory below.\n",
    "\n",
    "Similar to the last workshop, we will use both a prompt template and a LangChain LLM for this example. Note that this time our prompt template includes a `{history}` variable where our chat history will be included to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e48e507d-89be-4e1f-87b7-0fa1f7ffb187",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "CHAT_PROMPT_TEMPLATE = '''You are a helpful conversational assistant.\n",
    "{history}\n",
    "\n",
    "Human: {human_input}\n",
    "\n",
    "Assistant:\n",
    "'''\n",
    "PROMPT = PromptTemplate.from_template(CHAT_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961b106c-baa4-474c-97f9-eca7d868e52e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `Bedrock` was deprecated in LangChain 0.0.34 and will be removed in 0.3. An updated version of the class exists in the langchain-aws package and should be used instead. To use it run `pip install -U langchain-aws` and import as `from langchain_aws import BedrockLLM`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Bedrock\n",
    "\n",
    "llm = Bedrock(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24656b5",
   "metadata": {},
   "source": [
    "The `ConversationBufferMemory` class is instantiated here and you will notice that we use Claude specific human and assistant prefixes. When we initialize the memory, the history is blank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23e84d14-9b71-4235-83e3-045b87e6cbcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(human_prefix=\"\\nHuman\", ai_prefix=\"\\nAssistant\")\n",
    "history = memory.load_memory_variables({})['history']\n",
    "print(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217109c8",
   "metadata": {},
   "source": [
    "We now ask Claude a simple question \"How can I check for imbalances in my model?\". The LLM responds to the question and we can use the `add_user_message` and `add_ai_message` functions to save the input and output into memory. We can then retrieve the entire conversation history and print the response. Currently the model will still return answer using the data it was trained upon. Further will examine how to get a curated answer using our own FAq's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38515d05-6437-43fa-865e-1fa80b6c3fed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Human: How can I check for imbalances in my model?\n",
       "\n",
       "Assistant: Here are some things you can do to check for imbalances in your model:\n",
       "\n",
       "- Check the data used to train the model for biases or imbalances. Look at attributes like gender, race, age, location, etc. and ensure all groups are adequately represented. Imbalanced data can lead to models that don't generalize well.\n",
       "\n",
       "- Evaluate model performance on different demographic groups separately. Check that accuracy, error rates, etc. are similar across groups. Large differences could indicate unfairness. \n",
       "\n",
       "- Analyze model explanations/interpretations for potential biases. Look at what factors the model uses to make predictions and see if certain attributes have outsized influence in a harmful way.\n",
       "\n",
       "- Conduct fairness testing with tools like model cards, bias profiles, and fairness metrics. This helps systematically check for unfair treatment of individuals or groups.\n",
       "\n",
       "- Get feedback from diverse stakeholders on any potential harms or biases they see. Real users from varied backgrounds can help identify issues the model builders may have missed.  \n",
       "\n",
       "- Consider techniques like re-weighting the training process, regularization, or post-processing to reduce the influence of sensitive attributes on predictions. This can help address some imbalances.\n",
       "\n",
       "- Benchmark against other models, especially those designed specifically for fairness, to compare treatment of different subgroups.\n",
       "\n",
       "Regular monitoring and auditing of models can help catch issues with imbalances or unfairness over time as usage expands. Being diligent about inclusiveness in data and model performance is important."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_input = 'How can I check for imbalances in my model?'\n",
    "\n",
    "prompt_data = PROMPT.format(human_input=human_input, history=history)\n",
    "ai_output = llm(prompt_data)\n",
    "\n",
    "memory.chat_memory.add_user_message(human_input)\n",
    "memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "history = memory.load_memory_variables({})['history']\n",
    "display(Markdown(f'{history}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4df10c",
   "metadata": {},
   "source": [
    "Now we will ask a follow up question about the kind of imbalances does it detect and save the input and outputs again. Notice how the model is able to understand that when the human says \"it\", because it has access to the context of the chat history, the model is able to accurately understand what the user is asking about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b6b6f1-7101-4603-87a8-9b6b0032226d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " There are a few main types of imbalances that can be detected when checking a model:\n",
       "\n",
       "- Demographic imbalances - This refers to discrepancies in how well the model performs across different demographic groups like gender, race, age, location etc. Metrics here look at variable performance across subgroups.\n",
       "\n",
       "- Class imbalance - When one or more classes/labels are under-represented in the training data compared to others. This can skew the model to focus more on majority classes. \n",
       "\n",
       "- Attribute imbalance - Certain input attributes may have a disproportionately large influence on predictions in a way that harms some groups. Detecting dominant attributes is important.\n",
       "\n",
       "- Treatment imbalance - Related to demographic imbalances, but focuses on differences in how individuals from various subgroups are treated by the model. For example, higher error rates or unfair outcomes.\n",
       "\n",
       "- Selection bias - The data used to train the model may not properly represent the target population the model is intended to serve, neglecting certain subgroups. \n",
       "\n",
       "- Measurement bias - Errors or inconsistencies in how attributes are measured or defined could skew the modeling process unfairly against some individuals or demographics.\n",
       "\n",
       "So in summary, checks focus on variable performance, class/attribute distributions, influential factors, selection issues and potential unfair treatment when evaluating models for different types of imbalances. The goal is to ensure equitable, unbiased outcomes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "human_input = 'What kind does it detect?'\n",
    "\n",
    "prompt_data = PROMPT.format(human_input=human_input, history=history)\n",
    "ai_output = llm(prompt_data)\n",
    "\n",
    "memory.chat_memory.add_user_message(human_input)\n",
    "memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "#display(Markdown(f'{history}'))\n",
    "display(Markdown(f'{ai_output}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b107b4f-358b-4f29-887a-15ef5341abaf",
   "metadata": {},
   "source": [
    "---\n",
    "## Creating a class to help facilitate conversation\n",
    "\n",
    "To help create some structure around these conversations, we create a custom `Conversation` class below. This class will hold a stateful conversational memory and be the base for conversational RAG later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "396cac77-00a4-4ae2-913f-56ae1dbc60ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self, client, model_id: str=\"anthropic.claude-instant-v1\") -> None:\n",
    "        \"\"\"instantiates a new rag based conversation\n",
    "\n",
    "        Args:\n",
    "            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to \"anthropic.claude-instant-v1\".\n",
    "        \"\"\"\n",
    "\n",
    "        # instantiate memory\n",
    "        self.memory = ConversationBufferMemory(human_prefix=\"\\nHuman\", ai_prefix=\"\\nAssistant\")\n",
    "\n",
    "        # instantiate LLM connection\n",
    "        self.llm = Bedrock(\n",
    "            client=client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs={\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.9,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def ai_respond(self, user_input: str=None):\n",
    "        \"\"\"responds to the user input in the conversation with context used\n",
    "\n",
    "        Args:\n",
    "            user_input (str, optional): user input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ai_output (str): response from AI chatbot\n",
    "        \"\"\"\n",
    "\n",
    "        # format the prompt with chat history and user input\n",
    "        history = self.memory.load_memory_variables({})['history']\n",
    "        llm_input = PROMPT.format(history=history, human_input=user_input)\n",
    "\n",
    "        # respond to the user with the LLM\n",
    "        ai_output = self.llm(llm_input).strip()\n",
    "\n",
    "        # store the input and output\n",
    "        self.memory.chat_memory.add_user_message(user_input)\n",
    "        self.memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "        return ai_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a24eed",
   "metadata": {},
   "source": [
    "Let's see the class in action with two contextual questions. Again, notice the model is able to correctly interpret the context because it has memory of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154f8e9b-6088-4a65-859b-e6087b1949c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = Conversation(client=boto3_bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d35169-0dee-430e-9646-5d79dc713109",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are a few things you can do to check for imbalances in your machine learning model:\n",
       "\n",
       "- Compute demographic parity metrics like statistical parity or equalized odds. These quantify how well a model treats different demographic groups (e.g. gender, race) equally in terms of predicted outcomes. Imbalances will show up as differences across groups.\n",
       "\n",
       "- Look at precision, recall and F1 scores stratified by demographic groups. Imbalances may show up as differences in these evaluation metrics across groups. For example, higher recall but lower precision for one group.\n",
       "\n",
       "- Visualize the model's predictions on a sample of test data and check if it behaves similarly across demographic attributes. Look for inconsistencies or bias against certain groups.\n",
       "\n",
       "- audit model explanations/attentions to see if they rely on protected attributes like gender or race when making predictions. Reliance is a sign of potential unfair bias.\n",
       "\n",
       "- collect bias metrics like disparate impact which quantify differences in false positive/negative rates between groups. Imbalance is indicated by disparities. \n",
       "\n",
       "- reweight or resample the training data to create demographic parity, then retrain and check if overall accuracy decreases significantly. Large drops suggest overdependence on imbalanced attributes.\n",
       "\n",
       "The goal is to systematically analyze model performance, explanations and outcomes across demographic slices to detect any harmful biases or imbalances during development. Continual monitoring after deployment is also important."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = chat.ai_respond('How can I check for imbalances in my model?')\n",
    "display(Markdown(f'{output}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43fb63f8-d8fc-4a4e-9bc3-6ccb1c30d002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here are some common types of imbalances that can be detected when checking a machine learning model:\n",
       "\n",
       "- Statistical imbalances - When the distribution of labels/classes is uneven across different demographic groups in the training data. This can result in poorer performance for under-represented groups.\n",
       "\n",
       "- Precision/recall imbalances - If a model has significantly higher precision but lower recall (or vice versa) for certain groups. Indicates some groups may be over or under-predicted.  \n",
       "\n",
       "- Threshold imbalances - Differences in decision thresholds applied by the model across groups can cause disparities. For example, higher thresholds for one group.\n",
       "\n",
       "- Feature reliance imbalances - If model explanations or feature importances reveal stronger reliance on certain features correlated with protected attributes like gender or race. \n",
       "\n",
       "- Disparate treatment - When individuals from different demographic groups with similar characteristics receive different treatment or predictions from the model.\n",
       "\n",
       "- Disparate impact - When a model predicts equal treatment but results in unequal adverse impact or harm between demographic groups. For example, higher error rates for a particular group.\n",
       "\n",
       "- Masking and redirection biases - When a model learns to mask or redirect predictions based on explicit/implicit cues about an individual's demographic group rather than their qualifications.\n",
       "\n",
       "Regular auditing helps uncover statistical, predictive performance, decision threshold, reliance, treatment and impact imbalances between groups. This ensures fair and representative modeling."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = chat.ai_respond('What kind does it detect?')\n",
    "display(Markdown(f'{output}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192a46c-5a1f-4683-8c66-c47f05477468",
   "metadata": {},
   "source": [
    "---\n",
    "## Combining RAG with Conversation\n",
    "\n",
    "Now that we have a conversational system built, lets incorporate the RAG system we built in notebook 02 into the chat paradigm. \n",
    "\n",
    "First, we will create the same vector store with LangChain and FAISS from the last notebook.\n",
    "\n",
    "Our goal is to create a curated response from the model and only use the FAQ's we have provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23b07c75-896c-4a99-b214-ad42c12651e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open faiss-index/langchain/index.faiss for reading: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10459/826021851.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"amazon.titan-embed-text-v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# create vector store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./faiss-index/langchain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dangerous_deserialization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cls, folder_path, embeddings, index_name, allow_dangerous_deserialization, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m             )\n\u001b[1;32m   1090\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# load index separately since it is not picklable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0mfaiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependable_faiss_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.faiss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# load docstore and index_to_docstore_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34mf\"{index_name}.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/faiss/swigfaiss_avx512.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  10537\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx512\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open faiss-index/langchain/index.faiss for reading: No such file or directory"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# create instantiation to embedding model\n",
    "embedding_model = BedrockEmbeddings(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# create vector store\n",
    "vs = FAISS.load_local('./faiss-index/langchain', embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb169d",
   "metadata": {},
   "source": [
    "### Visualize Semantic Search \n",
    "\n",
    "⚠️ ⚠️ ⚠️ This section is for Advanced Practioners. Please feel free to run through these cells and come back later to re-examine the concepts ⚠️ ⚠️ ⚠️ \n",
    "\n",
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store\n",
    "\n",
    "\n",
    "##### Citation\n",
    "We will also be able to get the `citation` or the underlying documents which our Vector Store matched to our query. This is useful for debugging and also measuring the quality of the vector stores. let us look at how the underlying Vector store calculates the matches\n",
    "\n",
    "##### Vector DB Indexes\n",
    "One of the key components of the Vector DB is to be able to retrieve documents matching the query with accuracy and speed. There are multiple algorithims for the same and some examples can be [read here](https://thedataquarry.com/posts/vector-db-3/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3071f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#- helpful function to display in tabular format\n",
    "\n",
    "def display_table(data):\n",
    "    html = \"<table>\"\n",
    "    for row in data:\n",
    "        html += \"<tr>\"\n",
    "        for field in row:\n",
    "            html += \"<td>%s</td>\"%(field)\n",
    "        html += \"</tr>\"\n",
    "    html += \"</table>\"\n",
    "    display(HTML(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0dd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "v = embedding_model.embed_query(\"How can I check for imbalances in my model?\")\n",
    "print(v[0:10])\n",
    "results = vs.similarity_search_by_vector(v, k=2)\n",
    "display(Markdown('Let us look at the documents which had the relevant information pertaining to our query'))\n",
    "for r in results:\n",
    "    display(Markdown(f'{r.page_content}'))\n",
    "    display(Markdown(f'------------------------------------'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dbe79e",
   "metadata": {},
   "source": [
    "#### Similarity Search\n",
    "\n",
    "##### Distance scoring in Vector Data bases\n",
    "[Distance scores](https://weaviate.io/blog/distance-metrics-in-vector-search) are the key in vector searches. Here are some FAISS specific methods. One of them is similarity_search_with_score, which allows you to return not only the documents but also the distance score of the query to them. The returned distance score is L2 distance ( Squared Euclidean) . Therefore, a lower score is better. Further in FAISS we have similarity_search_with_score (ranked by distance: low to high) and similarity_search_with_relevance_scores ( ranked by relevance: high to low) with both using the distance strategy. The similarity_search_with_relevance_scores calculates the relevance score as 1 - score. For more details of the various distance scores [read here](https://milvus.io/docs/metric.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a703b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"##### Let us look at the documents based on {vs.distance_strategy.name} which will be used to answer our question 'What kind of bias does Clarify detect ?'\"))\n",
    "\n",
    "context = vs.similarity_search('What kind of bias does Clarify detect ?', k=2)\n",
    "#-  langchain.schema.document.Document\n",
    "display(Markdown(f'------------------------------------'))\n",
    "list_context = [[doc.page_content, doc.metadata] for doc in context]\n",
    "list_context.insert(0, ['Documents', 'Meta-data'])\n",
    "display_table(list_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a356d",
   "metadata": {},
   "source": [
    "Let us first look at the Page context and the meta data associated with the documents. Now let us look at the L2 scores based on the distance scoring as explained above. Lower score is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- relevancy of the documents\n",
    "results = vs.similarity_search_with_score(\"What kind of bias does Clarify detect ?\", k=2, fetch_k=3)\n",
    "display(Markdown(f'##### Similarity Search Table with relevancy score.'))\n",
    "display(Markdown(f'------------------------------------'))   \n",
    "results.insert(0,['Documents', 'Relevancy Score'])\n",
    "display_table(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f461678",
   "metadata": {},
   "source": [
    "#### Marginal Relevancy score\n",
    "\n",
    "Maximal Marginal Relevance  has been introduced in the paper [The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries](https://www.cs.cmu.edu/~jgc/publication/The_Use_MMR_Diversity_Based_LTMIR_1998.pdf). Maximal Marginal Relevance tries to reduce the redundancy of results while at the same time maintaining query relevance of results for already ranked documents/phrases etc. In the below results since we have a very limited data set it might not make a difference but for larger data sets the query will theoritically run faster while still preserving the over all relevancy of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#- normalizing the relevancy\n",
    "display(Markdown('##### Let us look at MRR scores'))\n",
    "results = vs.max_marginal_relevance_search_with_score_by_vector(embedding_model.embed_query(\"What kind of bias does Clarify detect ?\"), k=3)\n",
    "results.insert(0, [\"Document\", \"MRR Score\"])\n",
    "display_table(results)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ad9bb",
   "metadata": {},
   "source": [
    "#### Update embeddings of the Vector Databases\n",
    "\n",
    "Update of documents happens all the time and we have multiple versions of the documents. Which means we need to also factor how do we update the embeddings in our Vector Data bases. Fortunately we have and can leverage the meta data to update embeddings\n",
    "\n",
    "The key steps are:\n",
    "1. Load the new embeddings and add the meta data stating the version as 2\n",
    "2. Merge to the exisiting Vector database\n",
    "3. Run the query using the filter to only search in the new index and get the latest documents for the same query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7346ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector store\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "loader = CSVLoader(\n",
    "    file_path=\"./data/sagemaker/sm_faq_v2.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": [\"Question\", \"Answer\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "#docs_split = loader.load()\n",
    "docs_split = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0, separator=\",\").split_documents(loader.load())\n",
    "list_of_documents = [Document(page_content=doc.page_content, metadata=dict(page='v2')) for idx, doc in enumerate(docs_split)]\n",
    "print(f\"Number of split docs={len(docs_split)}\")\n",
    "db = FAISS.from_documents(list_of_documents, embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc3afa",
   "metadata": {},
   "source": [
    "#### Run a query against version 2 of the documents\n",
    "Let us run the query agsint our exisiting vector data base and we will see the the exisiting or the version 1 of the documents coming back. If we run with the filter since those do not exist in our vector Database we will see no results returned or an empty list back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the query with requesting data from version 2 which does not exist\n",
    "vs = FAISS.load_local('./faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)\n",
    "search_query = \"How can I check for imbalances in my model?\"\n",
    "#print(f\"Running with v1 of the documents we get response of {vs.similarity_search_with_score(query=search_query, k=1, fetch_k=4)}\")\n",
    "print(\"------\\n\")\n",
    "print(f\"Running the query with V2 of the document we get {vs.similarity_search_with_score(query=search_query, filter=dict(page='v2'), k=1)}:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f786cbc",
   "metadata": {},
   "source": [
    "#### Add a new version of the document\n",
    "We will create the version 2 of the documents and use meta data to add to our original index. Once done we will then apply a filter in our query which will return to us the documents newly added. Run the query now after adding version of the documents\n",
    "\n",
    "We will also examine a way to speed up our searches and queries and look at another way to narrow the search using the  fetch_k parameter when calling similarity_search with filters. Usually you would want the fetch_k to be more than the k parameter. This is because the fetch_k parameter is the number of documents that will be fetched before filtering. If you set fetch_k to a low number, you might not get enough documents to filter from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6a33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - now let us add version 2 of the data set and run query from that\n",
    "\n",
    "vs.merge_from(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ef172",
   "metadata": {},
   "source": [
    "#### Query complete merged data base with no filters\n",
    "Run the query against the fully merged DB without any filters for the meta data and we see that it returns the top results of the new V2 data and also the top results of the v1 data. Essentially it will match and return data closest to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58008b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - run the query again\n",
    "search_query_v2 = \"How can I check for imbalances in my model?\"\n",
    "results_with_scores = vs.similarity_search_with_score(search_query_v2, k=2, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199c42f",
   "metadata": {},
   "source": [
    "#### Query with Filter\n",
    "Now we will ask to search only against the version 2 of the data and use filter criteria against it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4c46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - run the query again\n",
    "search_query_v2 = \"How can I check for imbalances in my model?\"\n",
    "results_with_scores = vs.similarity_search_with_score(search_query_v2, filter=dict(page='v2'), k=2, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f761b4bc",
   "metadata": {},
   "source": [
    "#### Query for new data\n",
    "Now let us ask a question which exists only on the version 2 of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9351155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - now let us ask a question which ONLY exits in the version 2 of the document\n",
    "search_query_v2 = \"Can i use Quantum computing?\"\n",
    "results_with_scores = vs.similarity_search_with_score(query=search_query_v2, filter=dict(page='v2'), k=1, fetch_k=3)\n",
    "results_with_scores = [[doc.page_content, doc.metadata, score] for doc, score in results_with_scores]\n",
    "results_with_scores.insert(0, ['Document', 'Meta-Data', 'Score'])\n",
    "display_table(results_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a8c52b",
   "metadata": {},
   "source": [
    "### Let us continue to build our chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c61cab4",
   "metadata": {},
   "source": [
    "The prompt template is now altered to include both conversation memory as well as chat history as inputs along with the human input. Notice how the prompt also instructs Claude to not answer questions which it does not have the context for. This helps reduce hallucinations which is extremely important when creating end user facing applications which need to be factual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efedb094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-create vector store and continue\n",
    "vs = FAISS.load_local('./faiss-index/langchain/', embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ac933-0cfb-4982-997b-f43365b7bf84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RAG_TEMPLATE = \"\"\"You are a helpful conversational assistant.\n",
    "\n",
    "If you are unsure about the answer OR the answer does not exist in the context, respond with\n",
    "\"Sorry but I do not understand your request. I am still learning so I appreciate your patience! 😊\n",
    "NEVER make up the answer.\n",
    "\n",
    "If the human greets you, simply introduce yourself.\n",
    "\n",
    "The context will be placed in <context></context> XML tags. \n",
    "\n",
    "<context>{context}</context>\n",
    "\n",
    "Do not include any xml tags in your response.\n",
    "\n",
    "{history}\n",
    "\n",
    "Human: {input}\n",
    "\n",
    "Assistant:\n",
    "\"\"\"\n",
    "PROMPT = PromptTemplate.from_template(RAG_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f5867",
   "metadata": {},
   "source": [
    "The new `ConversationWithRetrieval` class now includes a `get_context` function which searches our vector database based on the human input and combines it into the base prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c905e61-fc05-400d-9efb-5b42b151b5a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConversationWithRetrieval:\n",
    "    def __init__(self, client, vector_store: FAISS=None, model_id: str=\"anthropic.claude-instant-v1\") -> None:\n",
    "        \"\"\"instantiates a new rag based conversation\n",
    "\n",
    "        Args:\n",
    "            vector_store (FAISS, optional): pre-populated vector store for searching context. Defaults to None.\n",
    "            model_id (str, optional): which bedrock model to use for the conversational agent. Defaults to \"anthropic.claude-instant-v1\".\n",
    "        \"\"\"\n",
    "\n",
    "        # store vector store\n",
    "        self.vector_store = vector_store\n",
    "        \n",
    "        # instantiate memory\n",
    "        self.memory = ConversationBufferMemory(human_prefix=\"Human\", ai_prefix=\"Assistant\")\n",
    "\n",
    "        # instantiate LLM connection\n",
    "        self.llm = Bedrock(\n",
    "            client=client,\n",
    "            model_id=model_id,\n",
    "            model_kwargs={\n",
    "                \"max_tokens_to_sample\": 500,\n",
    "                \"temperature\": 0.0,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def ai_respond(self, user_input: str=None):\n",
    "        \"\"\"responds to the user input in the conversation with context used\n",
    "\n",
    "        Args:\n",
    "            user_input (str, optional): user input. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            ai_output (str): response from AI chatbot\n",
    "            search_results (list): context used in the completion\n",
    "        \"\"\"\n",
    "\n",
    "        # format the prompt with chat history and user input\n",
    "        context_string, search_results = self.get_context(user_input)\n",
    "        history = self.memory.load_memory_variables({})['history']\n",
    "        llm_input = PROMPT.format(history=history, input=user_input, context=context_string)\n",
    "\n",
    "        # respond to the user with the LLM\n",
    "        ai_output = self.llm(llm_input).strip()\n",
    "\n",
    "        # store the input and output\n",
    "        self.memory.chat_memory.add_user_message(user_input)\n",
    "        self.memory.chat_memory.add_ai_message(ai_output.strip())\n",
    "\n",
    "        return ai_output, search_results\n",
    "\n",
    "    def get_context(self, user_input, k=5):\n",
    "        \"\"\"returns context used in the completion\n",
    "\n",
    "        Args:\n",
    "            user_input (str): user input as a string\n",
    "            k (int, optional): number of results to return. Defaults to 5.\n",
    "\n",
    "        Returns:\n",
    "            context_string (str): context used in the completion as a string\n",
    "            search_results (list): context used in the completion as a list of Document objects\n",
    "        \"\"\"\n",
    "        search_results = self.vector_store.similarity_search(\n",
    "            user_input, k=k\n",
    "        )\n",
    "        context_string = '\\n\\n'.join([f'Document {ind+1}: ' + i.page_content for ind, i in enumerate(search_results)])\n",
    "        return context_string, search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd0bbee",
   "metadata": {},
   "source": [
    "Now the model can answer some specific domain questions based on our document database!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282d088-aa3c-493e-b1f4-0d47c27ffed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chat = ConversationWithRetrieval(boto3_bedrock, vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaaad87-48fe-495f-98d9-4c7cd95ad5b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output, context = chat.ai_respond('How can I check for imbalances in my model?')\n",
    "display(Markdown(f'{output}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e995518-35e8-4106-b454-fc443c68f1f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output, context = chat.ai_respond('What kind does it detect?')\n",
    "display(Markdown(f'** Ai Assistant Answer: ** \\n{output}'))\n",
    "display(Markdown(f'\\n\\n** Relevant Documentation: ** \\n{context}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c22f06",
   "metadata": {},
   "source": [
    "--- \n",
    "## Using LangChain for Orchestration of RAG\n",
    "\n",
    "Beyond the primitive classes for prompt handling and conversational memory management, LangChain also provides a framework for [orchestrating RAG flows](https://python.langchain.com/docs/expression_language/cookbook/retrieval) with what purpose built \"chains\". In this section, we will see how to be a retrieval chain with LangChain which is more comprehensive and robust than the original retrieval system we built above.\n",
    "\n",
    "The workflow we used above follows the following process...\n",
    "\n",
    "1. User input is received.\n",
    "2. User input is queried against the vector database to retrieve relevant documents.\n",
    "3. Relevant documents and chat memory are inserted into a new prompt to respond to the user input.\n",
    "4. Return to step 1.\n",
    "\n",
    "However, more complex methods of interacting with the user input can generate more accurate results in RAG architectures. One of the popular mechanisms which can increase accuracy of these retrieval systems is utilizing more than one call to an LLM in order to reformat the user input for more effective search to your vector database. A better workflow is described below compared to the one we already built...\n",
    "\n",
    "1. User input is received.\n",
    "2. An LLM is used to reword the user input to be a better search query for the vector database based on the chat history and other instructions. This could include things like condensing, rewording, addition of chat context, or stylistic changes.\n",
    "3. Reformatted user input is queried against the vector database to retrieve relevant documents.\n",
    "4. The reformatted user input and relevant documents are inserted into a new prompt in order to answer the user question.\n",
    "5. Return to step 1.\n",
    "\n",
    "Let's now build out this second workflow using LangChain below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8611a13a",
   "metadata": {},
   "source": [
    "First we need to make a prompt which will reformat the user input to be more compatible for searching of the vector database. The way we do this is by providing the chat history as well as the some basic instructions to Claude and asking it to condense the input into a single output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e314e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-message>\n",
    "{question}\n",
    "<follow-up-message>\n",
    "\n",
    "Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, \\\n",
    "rewrite the follow up message to be a standalone question that captures all relevant context \\\n",
    "from the conversation. Answer only with the new question and nothing else.\n",
    "\n",
    "Assistant: Standalone Question:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d7099",
   "metadata": {},
   "source": [
    "The next prompt we need is the prompt which will answer the user's question based on the retrieved information. In this case, we provide specific instructions about how to answer the question as well as provide the context retrieved from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d854a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "respond_prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Human: Given the context above, answer the question inside the <q></q> XML tags.\n",
    "\n",
    "<q>{question}</q>\n",
    "\n",
    "If the answer is not in the context say \"Sorry, I don't know as the answer was not found in the context\". Do not use any XML tags in the answer.\n",
    "\n",
    "Assistant:\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f9fbf",
   "metadata": {},
   "source": [
    "Now that we have our prompts set up, let's set up the conversational memory buffer just like we did earlier in the notebook. Notice how we inject an example human and assistant message in order to help guide our AI assistant on what its job is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920aefbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Bedrock(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\"max_tokens_to_sample\": 500, \"temperature\": 0.9}\n",
    ")\n",
    "memory_chain = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    human_prefix=\"Human\",\n",
    "    ai_prefix=\"Assistant\"\n",
    ")\n",
    "memory_chain.chat_memory.add_user_message(\n",
    "    'Hello, what are you able to do?'\n",
    ")\n",
    "memory_chain.chat_memory.add_ai_message(\n",
    "    'Hi! I am a help chat assistant which can answer questions about Amazon SageMaker.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b223df37",
   "metadata": {},
   "source": [
    "Lastly, we will used the `ConversationalRetrievalChain` from LangChain to orchestrate this whole system. If you would like to see some more logs about what is happening in the orchestration and not just the final output, make sure to change the `verbose` argument to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5da087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, # this is our claude model\n",
    "    retriever=vs.as_retriever(), # this is our FAISS vector database\n",
    "    memory=memory_chain, # this is the conversational memory storage class\n",
    "    condense_question_prompt=condense_prompt, # this is the prompt for condensing user inputs\n",
    "    verbose=False, # change this to True in order to see the logs working in the background\n",
    ")\n",
    "qa.combine_docs_chain.llm_chain.prompt = respond_prompt # this is the prompt in order to respond to condensed questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf46fba3",
   "metadata": {},
   "source": [
    "Let's go ahead and generate some responses from our RAG solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d10f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"{qa.run({'question': 'How can I check for imbalances in my model?'})}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af21687",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"{qa.run({'question': 'What kind does it detect?' })}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61caec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"{qa.run({'question': 'How does this improve model explainability?' })}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c11819",
   "metadata": {},
   "source": [
    "## Let us use LLM to validate if the response was factual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68954164",
   "metadata": {},
   "source": [
    "#### We first create a sanity prompt, which will use the vector DB results and ask the LLM to validate if the respinse given was acurate or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf776d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sanity check prompt\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def create_sanity_prompt(instruction_start: str = None,instruction_end: str = None,) -> PromptTemplate:\n",
    "    \"\"\"\n",
    "    Create a prompt template for LLM sanity check\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    instruction_start : str, optional\n",
    "        Instrcution in the beginning of the prompt, by default None\n",
    "    instruction_end : str, optional\n",
    "        Instrcution in the end of the prompt, by default None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PromptTemplate\n",
    "        Prompt template in the LangChain format\n",
    "    \"\"\"\n",
    "\n",
    "    # first instruction\n",
    "    prompt_template_build = instruction_start + \"\\n\"\n",
    "\n",
    "    # add context\n",
    "    prompt_template_build += \"Context: {context}\" + \"\\n\"\n",
    "\n",
    "    # add statement\n",
    "    prompt_template_build += \"Statement: {statement}\" + \"\\n\"\n",
    "\n",
    "    # addinstruction\n",
    "    prompt_template_build += \"Question: \" + instruction_end + \"\\n\"\n",
    "\n",
    "    # add answer placeholder\n",
    "    prompt_template_build += \"Answer:\"\n",
    "\n",
    "    print(prompt_template_build)\n",
    "    # build the template\n",
    "    llm_prompt = PromptTemplate(\n",
    "        template=str(prompt_template_build),\n",
    "        input_variables=[\"context\", \"statement\"],\n",
    "    )\n",
    "    return llm_prompt\n",
    "\n",
    "sanity_prompt = create_sanity_prompt(\n",
    "    instruction_start=\"\"\"The following is a conversation between a highly knowledgeable and intelligent AI assistant, called Falcon, and a human user asking Questions. In the following interactions, Falcon will converse in natural language, and Falcon will answer the questions based only on the provided Context. Falcon will provide accurate, short and direct answers to the questions.\"\"\",\n",
    "    instruction_end=\"Is the above statement based directly on the provided context? Answer with yes or no.\",\n",
    ")\n",
    "\n",
    "docs = vs.similarity_search_with_score('How can I check for imbalances in my model?')\n",
    "contexts = []\n",
    "source = []\n",
    "for doc, score in docs:\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")\n",
    "    if score <= 0.9:\n",
    "        contexts.append(doc)\n",
    "        source.append(doc.metadata['source'])\n",
    "        print(f\"\\n INPUT CONTEXT:{contexts}\")\n",
    "\n",
    "sanity_chain = load_qa_chain(llm=llm, prompt=sanity_prompt)\n",
    "sanity_check = sanity_chain({\"input_documents\": contexts, \"statement\": output},return_only_outputs=True)['output_text']\n",
    "\n",
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a0b01f",
   "metadata": {},
   "source": [
    "#### We can see the Vector database responded accurately to our query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38c881",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502732a",
   "metadata": {},
   "source": [
    "--- \n",
    "## Using LlamaIndex for Orchestration of RAG\n",
    "\n",
    "Another popular open source framework for orchestrating RAG is [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/index.html). Let's take a look below at how to use our SageMaker FAQ vector index to have a conversational RAG application with LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c059f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "from llama_index import ServiceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69279fe5",
   "metadata": {},
   "source": [
    "First we need to set up the system setting to define the embedding model and LLM. Again, we will be using titan and claude respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2677f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = BedrockEmbeddings(client=boto3_bedrock, model_id=\"amazon.titan-embed-text-v1\")\n",
    "llm = Bedrock(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=\"anthropic.claude-instant-v1\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 500,\n",
    "        \"temperature\": 0.9,\n",
    "    },\n",
    ")\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm, embed_model=embed_model, chunk_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80d5083",
   "metadata": {},
   "source": [
    "The next step would be to create a FAISS index from our document base. In this lab, this is already done for you and stored in the [faiss-index/llama-index/](../faiss-index/llama-index/) folder.\n",
    "\n",
    "If you are interested in how this was accomplished, follow [this tutorial](https://gpt-index.readthedocs.io/en/latest/examples/vector_stores/FaissIndexDemo.html) from LlamIndex. The code below is the basics of how this was accomplished as well.\n",
    "\n",
    "```python\n",
    "# faiss_index = faiss.IndexFlatL2(1536)\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# documents = SimpleDirectoryReader(\"./../data/sagemaker\").load_data()\n",
    "# vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents, storage_context=storage_context, service_context=service_context\n",
    "# )\n",
    "# index.storage_context.persist('../faiss-index/llama-index/')\n",
    "```\n",
    "\n",
    "Once the index is created, we can load the persistent files to a `FaissVectorStore` object and create a `query_engine` from the vector index. To learn more about indicies in LlamaIndex, read more [here](https://gpt-index.readthedocs.io/en/latest/understanding/indexing/indexing.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66c68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import load_index_from_storage, StorageContext\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "\n",
    "vector_store = FaissVectorStore.from_persist_dir(\"./faiss-index/llama-index\")\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store, persist_dir=\"./faiss-index/llama-index\"\n",
    ")\n",
    "index = load_index_from_storage(storage_context=storage_context, service_context=service_context)\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92161d17",
   "metadata": {},
   "source": [
    "Now let's set up a retrieval based chat application similar to LangChain. We will use the same condensing question strategy as before and can reuse the same prompt to condense the question for vector searching. Notice how we include some custom chat history to inject context into the prompt for the model to understand what we are asking questions about. The resulting `chat_engine` object is now fully ready to chat about our documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942c2baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts  import PromptTemplate\n",
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine\n",
    "\n",
    "custom_prompt = PromptTemplate(\"\"\"\\\n",
    "<chat-history>\n",
    "{chat_history}\n",
    "</chat-history>\n",
    "\n",
    "<follow-up-message>\n",
    "{question}\n",
    "<follow-up-message>\n",
    "\n",
    "Human: Given the conversation above (between Human and Assistant) and the follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation. Answer only with the new question and nothing else.\n",
    "\n",
    "Assistant: Standalone Question:\"\"\")\n",
    "\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content='Hello assistant, I have some questions about using Amazon SageMaker today.'\n",
    "    ),\n",
    "    ChatMessage(\n",
    "        role=MessageRole.ASSISTANT,\n",
    "        content='Okay, sounds good.'\n",
    "    )\n",
    "]\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    service_context=service_context,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c59d3ba",
   "metadata": {},
   "source": [
    "Let's go ahead and ask our first question. Notice that the verbose `chat_engine` will print out the condensed question as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a330c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"How can I check for imbalances in my model?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e737e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd137da",
   "metadata": {},
   "source": [
    "Now follow up questions can be asked with conversational context in mind!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d2d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"How does this improve model explainability?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f4f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a48a0e8-147d-4525-a6b2-68a09af1b2c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Next steps\n",
    "\n",
    "Now that we have a working RAG application with vector search retrieval, we will explore a new type of retrieval. In the next notebook we will see how to use LLM agents to automatically retrieve information from APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f224b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccda9787",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34014f07",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
